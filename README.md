# ExDRoP - Extrator de Dados de Royalties do PetrÃ³leo

## ğŸ“œ Sobre o Projeto

O **ExDRoP** Ã© uma ferramenta de automaÃ§Ã£o projetada para extrair, de forma robusta e paralela, dados de pagamentos de royalties de mÃºltiplos portais da transparÃªncia de municÃ­pios em Sergipe. O projeto nasceu da necessidade de contornar a falta de padronizaÃ§Ã£o e a dificuldade de acesso a esses dados pÃºblicos, fornecendo um conjunto de dados limpo e consolidado para anÃ¡lise e controle social.

O sistema conta com uma interface grÃ¡fica amigÃ¡vel para configuraÃ§Ã£o e execuÃ§Ã£o, alÃ©m de um modo de automaÃ§Ã£o para uso em servidores.

## âœ¨ Funcionalidades

* **ExtraÃ§Ã£o Multi-portal:** Suporte para diferentes layouts de portais (famÃ­lia Serigy e outros).
* **Processamento Paralelo:** Utiliza mÃºltiplos "workers" (threads) para acelerar drasticamente a extraÃ§Ã£o de dados.
* **Robusto e Resiliente:** Possui lÃ³gicas de retentativas para lidar com instabilidades dos portais, coleta em lotes para evitar travamentos e salvamento de diagnÃ³stico em caso de falhas.
* **Interface GrÃ¡fica:** Um painel de controle construÃ­do com Streamlit permite a configuraÃ§Ã£o e o acompanhamento da extraÃ§Ã£o em tempo real, de forma totalmente visual.
* **Containerizado com Docker:** Tanto o scraper quanto a interface podem ser executados via Docker, garantindo um ambiente consistente e facilitando a distribuiÃ§Ã£o.
* **ConfiguraÃ§Ã£o FlexÃ­vel:** Todas as execuÃ§Ãµes sÃ£o controladas por um arquivo `config.json`, permitindo selecionar cidades, anos e meses especÃ­ficos.

## ğŸ› ï¸ PrÃ©-requisitos

Para executar o projeto, vocÃª precisarÃ¡ de:

* **Python 3.11+**
* **Docker Desktop** (para a execuÃ§Ã£o via containers, que Ã© a forma mais recomendada)

## ğŸš€ Como Executar o ExDRoP

Este projeto oferece mÃºltiplas formas de execuÃ§Ã£o, projetadas para diferentes tipos de usuÃ¡rios, desde pesquisadores que desejam uma interface amigÃ¡vel atÃ© desenvolvedores que precisam de automaÃ§Ã£o em servidor.

### Modo 1: Usando a Interface GrÃ¡fica (Recomendado para UsuÃ¡rios)

Esta Ã© a forma mais fÃ¡cil de usar o extrator. Ela abre um painel de controle no seu navegador onde vocÃª pode configurar e iniciar a extraÃ§Ã£o.

#### OpÃ§Ã£o A (Mais Simples): Via Docker
VocÃª sÃ³ precisa do Docker Desktop instalado no seu computador.

1.  **Construa a imagem da UI (apenas na primeira vez):**
    ```bash
    # O -f aponta para o Dockerfile da interface
    docker build -t extrator-ui -f Dockerfile.ui .
    ```

2.  **Execute o container da UI:**
    ```powershell
    # Comando para Windows (PowerShell)
    docker run --rm -p 8501:8501 -v "$(pwd)/data:/app/data" -v "$(pwd)/logs:/app/logs" -v "$(pwd)/config.json:/app/config.json" extrator-ui
    ```

3.  **Acesse a Interface:**
    Abra seu navegador e acesse: **`http://localhost:8501`**

#### OpÃ§Ã£o B (Para Desenvolvimento): Localmente com `venv`
Este mÃ©todo Ã© ideal para quem estÃ¡ a modificar o cÃ³digo da interface.

1.  **Ative o ambiente virtual:**
    ```powershell
    # Para Windows (PowerShell)
    .\venv\Scripts\Activate
    ```
2.  **Instale as dependÃªncias (se necessÃ¡rio):**
    ```bash
    pip install -r requirements.txt
    ```
3.  **Execute o Streamlit:**
    ```bash
    python -m streamlit run interface.py
    ```

### Modo 2: ExecuÃ§Ã£o Automatizada do Scraper (Recomendado para Servidores)

Este modo executa o robÃ´ em "headless" (sem interface grÃ¡fica), lendo a configuraÃ§Ã£o do `config.json`. Ã‰ ideal para ser agendado (`cron`) em um servidor.

1.  **Construa a imagem do scraper (apenas na primeira vez):**
    ```bash
    # O -f aponta para o Dockerfile do scraper
    docker build -t extrator-sergipe -f Dockerfile.scraper .
    ```

2.  **Execute o container do scraper:**
    ```powershell
    # Comando para Windows (PowerShell)
    docker run --rm -v "$(pwd)/data:/app/data" -v "$(pwd)/logs:/app/logs" -v "$(pwd)/config.json:/app/config.json" extrator-sergipe
    ```

    ## ğŸ“‚ Estrutura do Projeto
```
etl-transparencia-sergipe
â”œâ”€ .editorconfig
â”œâ”€ config.json
â”œâ”€ Dockerfile.scraper
â”œâ”€ Dockerfile.ui
â”œâ”€ docs
â”‚  â””â”€ notebooks
â”‚     â”œâ”€ data_science.ipynb
â”‚     â”œâ”€ OSR_aracaju_barra__pirambu.ipynb
â”‚     â”œâ”€ OSR_pacatuba.ipynb
â”‚     â””â”€ teste_selenium.ipynb
â”œâ”€ estrutura_do_projeto.txt
â”œâ”€ interface.py
â”œâ”€ LICENSE
â”œâ”€ main.py
â”œâ”€ metricas_radon.md
â”œâ”€ README.md
â”œâ”€ requirements.in
â”œâ”€ requirements.txt
â””â”€ src
   â”œâ”€ common
   â”‚  â”œâ”€ file_utils.py
   â”‚  â”œâ”€ logging_setup.py
   â”‚  â””â”€ __init__.py
   â”œâ”€ scrapers
   â”‚  â”œâ”€ aracaju_barra_pirambu_scraper.py
   â”‚  â”œâ”€ pacatuba_scraper.py
   â”‚  â””â”€ __init__.py
   â””â”€ __init__.py

```

## âš™ï¸ ConfiguraÃ§Ã£o

A configuraÃ§Ã£o da extraÃ§Ã£o Ã© feita de forma simples e visual atravÃ©s da **Interface GrÃ¡fica**, nÃ£o sendo necessÃ¡rio editar arquivos manualmente para o uso comum.

### Configurando pela Interface GrÃ¡fica

Ao executar a interface (`Modo 1` da seÃ§Ã£o "Como Executar"), vocÃª encontrarÃ¡ as seguintes opÃ§Ãµes no painel de controle:

* **Prefeituras para Processar:** Uma caixa de seleÃ§Ã£o mÃºltipla para escolher quais municÃ­pios serÃ£o incluÃ­dos na extraÃ§Ã£o.
* **Anos para Processar:** Um campo de texto onde vocÃª pode listar os anos desejados, separados por vÃ­rgula.
* **Meses para Processar (Condicional):** Um campo de texto que aparece se vocÃª selecionar um municÃ­pio compatÃ­vel (todos os atuais). Permite especificar meses (ex: `01, 02, 11`), otimizando a extraÃ§Ã£o. Se deixado em branco, o robÃ´ processarÃ¡ o ano inteiro.
* **NÃºmero de Processos Paralelos (Workers):** Um slider para definir quantos navegadores rodarÃ£o simultaneamente. Uma ajuda (`?`) explica como escolher o melhor nÃºmero com base na capacidade do seu computador.
* **Executar em modo visual:** Uma caixa de seleÃ§Ã£o que permite assistir Ã  execuÃ§Ã£o do robÃ´. Ideal para depuraÃ§Ã£o.

Ao clicar em "Salvar ConfiguraÃ§Ãµes e Iniciar ExtraÃ§Ã£o", suas escolhas sÃ£o salvas automaticamente no arquivo `config.json` e a execuÃ§Ã£o comeÃ§a.

### Para Desenvolvedores ou AutomaÃ§Ã£o (Editando o `config.json`)

Para execuÃ§Ãµes automatizadas em servidor (`Modo 2`), a configuraÃ§Ã£o Ã© lida diretamente do arquivo `config.json`. VocÃª pode editÃ¡-lo manualmente para controlar o processo.

```json
{
  "anos_para_processar": ["2024", "2023"],
  "prefeituras_para_processar": ["aracaju", "pacatuba"],
  "meses_para_processar": ["01", "02", "03"],
  "configuracoes_paralelismo": {
    "max_workers": 4
  },
  "configuracoes_cidades": {
    "aracaju": {
      "scraper_module": "aracaju_barra_pirambu_scraper",
      "url": "[https://www.municipioonline.com.br/se/prefeitura/aracaju/cidadao/despesa](https://www.municipioonline.com.br/se/prefeitura/aracaju/cidadao/despesa)"
    },
    "barra": {
      "scraper_module": "aracaju_barra_pirambu_scraper",
      "url": "[https://www.municipioonline.com.br/se/prefeitura/barradoscoqueiros/cidadao/despesa](https://www.municipioonline.com.br/se/prefeitura/barradoscoqueiros/cidadao/despesa)"
    },
    "pirambu": {
      "scraper_module": "aracaju_barra_pirambu_scraper",
      "url": "[https://www.municipioonline.com.br/se/prefeitura/pirambu/cidadao/despesa](https://www.municipioonline.com.br/se/prefeitura/pirambu/cidadao/despesa)"
    },
    "pacatuba": {
      "scraper_module": "pacatuba_scraper",
      "url": "[https://transparencia.pacatuba.se.gov.br/public/portal/despesas](https://transparencia.pacatuba.se.gov.br/public/portal/despesas)"
    }
  }
}
```

* anos_para_processar: Lista de anos para os quais o robÃ´ irÃ¡ rodar.

* prefeituras_para_processar: Lista das chaves das cidades que serÃ£o processadas.

* meses_para_processar (Opcional): Se presente, o robÃ´ processarÃ¡ apenas os meses listados para as cidades compatÃ­veis. Se ausente ou null, processarÃ¡ o ano inteiro.

* max_workers: NÃºmero de tarefas paralelas (navegadores) a serem executadas ao mesmo tempo.

* configuracoes_cidades: DicionÃ¡rio com as configuraÃ§Ãµes especÃ­ficas de cada portal, como a URL e o mÃ³dulo scraper a ser utilizado.


## ğŸ“¦ ManutenÃ§Ã£o e AtualizaÃ§Ã£o das Imagens

Para garantir que a aplicaÃ§Ã£o continue segura e estÃ¡vel, Ã© recomendado reconstruir as imagens Docker periodicamente (a cada 1-2 meses) para incorporar as Ãºltimas atualizaÃ§Ãµes de seguranÃ§a da imagem base e das dependÃªncias.

O processo consiste em dois passos:

### 1. Atualizar a Imagem Base

Primeiro, garanta que vocÃª tenha a versÃ£o mais recente da imagem oficial do Python que usamos como base:

```bash
docker pull python:3.11-slim
```

### 2. Reconstruir as Imagens da AplicaÃ§Ã£o sem Cache
Em seguida, reconstrua as suas imagens `extrator-ui` e extrator-sergipe usando a flag --no-cache. Isso forÃ§a o Docker a executar todos os passos do zero, incluindo o apt-get upgrade, garantindo que as Ãºltimas atualizaÃ§Ãµes sejam aplicadas.

Para a imagem da Interface:

```bash

docker build --no-cache -t `extrator-ui` -f Dockerfile.ui .
```

Para a imagem do Scraper (automaÃ§Ã£o):

```bash

docker build --no-cache -t extrator-sergipe -f Dockerfile.scraper .
```


