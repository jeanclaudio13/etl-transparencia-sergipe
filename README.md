# Extrator de Dados de  do PetrÃ³leo - Portais da TransparÃªncia (Sergipe)

## ğŸ“– Sobre o Projeto

Este projeto contÃ©m um conjunto de robÃ´s (scrapers) desenvolvidos em Python para automatizar a extraÃ§Ã£o de dados de pagamentos de royalties dos portais da transparÃªncia de municÃ­pios de Sergipe.

A arquitetura foi projetada para ser modular, escalÃ¡vel e de fÃ¡cil manutenÃ§Ã£o, permitindo que novos scrapers para diferentes prefeituras sejam adicionados com o mÃ­nimo de esforÃ§o. A execuÃ§Ã£o Ã© feita em paralelo para otimizar o tempo de extraÃ§Ã£o.

## âœ¨ Funcionalidades Principais


* **Arquitetura Modular:** Cada tipo de portal (ex: Serigy, Ãgape) possui seu prÃ³prio mÃ³dulo de scraper, facilitando a manutenÃ§Ã£o e expansÃ£o.
* **ExecuÃ§Ã£o Paralela:** Utiliza mÃºltiplas threads para processar diferentes tarefas (meses ou anos) simultaneamente, acelerando drasticamente o tempo total da extraÃ§Ã£o.
* **Logging Detalhado:** Gera logs de execuÃ§Ã£o consolidados e identificados por tarefa, facilitando a depuraÃ§Ã£o e o monitoramento.
* ğŸ”¶ **(Em Desenvolvimento...) ExtraÃ§Ã£o Parametrizada:** Controle quais cidades e anos devem ser processados atravÃ©s de um Ãºnico arquivo de configuraÃ§Ã£o (`config.json`).
* ğŸ”¶ **(Em Desenvolvimento...) Ambiente Containerizado:** Empacotado com Docker para garantir um ambiente de execuÃ§Ã£o consistente e eliminar a necessidade de instalaÃ§Ãµes manuais na mÃ¡quina do cliente.

## ğŸ“‚ Estrutura do Projeto
**ğŸ”¶(Em Desenvolvimento...)** 

O projeto estÃ¡ organizado da seguinte forma para garantir a separaÃ§Ã£o de responsabilidades:

```
etl-transparencia-sergipe/
â”‚
â”œâ”€â”€ config.json                 # Arquivo para configurar a execuÃ§Ã£o
â”œâ”€â”€ Dockerfile                  # "Receita" para construir o container Docker
â”œâ”€â”€ main.py                     # Ponto de entrada principal do projeto
â”œâ”€â”€ README.md                   # Este arquivo
â”œâ”€â”€ requirements.txt            # Lista de bibliotecas Python
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ processed/              # Pasta para os arquivos CSV finais
â”‚       â”œâ”€â”€ aracaju/
â”‚       â””â”€â”€ pacatuba/
â”‚
â”œâ”€â”€ logs/                       # Pasta para os arquivos de log
â”‚
â””â”€â”€ src/                        # Pasta principal para todo o cÃ³digo-fonte
    â”œâ”€â”€ common/
    â”‚   â””â”€â”€ logging_setup.py
    â””â”€â”€ scrapers/
        â”œâ”€â”€ aracaju_barra_pirambu_scraper.py
        â””â”€â”€ pacatuba_scraper.py
```

## ğŸš€ Como Executar

Existem duas maneiras de executar o projeto: usando Docker (recomendado para clientes e produÃ§Ã£o) ou localmente (para desenvolvimento).

### PrÃ©-requisitos

* **Para execuÃ§Ã£o com Docker:** Ã‰ necessÃ¡rio apenas ter o [Docker Desktop](https://www.docker.com/products/docker-desktop/) instalado e em execuÃ§Ã£o.
* **Para desenvolvimento local:** Python 3.10+ e `pip`.

### Modo 1: ExecuÃ§Ã£o via Docker (Recomendado)

1.  **Construir a Imagem (feito apenas uma vez):**
    No terminal, na pasta raiz do projeto, execute:
    ```bash
    docker build -t extrator-sergipe .
    ```

2.  **Executar o Container:**
    Este comando inicia a extraÃ§Ã£o com base nas configuraÃ§Ãµes do `config.json`.
    ```bash
    # Para Windows (CMD ou PowerShell)
    docker run --rm -v "%cd%/data:/app/data" -v "%cd%/logs:/app/logs" extrator-sergipe

    # Para macOS ou Linux
    docker run --rm -v "$(pwd)/data:/app/data" -v "$(pwd)/logs:/app/logs" extrator-sergipe
    ```
    * Os arquivos CSV gerados aparecerÃ£o na pasta `data/processed` e os logs na pasta `logs`.

### Modo 2: ExecuÃ§Ã£o Local (Para Desenvolvimento)

1.  **Crie um Ambiente Virtual:**
    ```bash
    python -m venv venv
    ```

2.  **Ative o Ambiente Virtual:**
    ```bash
    # Windows
    .\venv\Scripts\activate
    # macOS/Linux
    source venv/bin/activate
    ```

3.  **Instale as DependÃªncias:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Execute o Script:**
    ```bash
    python main.py
    ```

## âš™ï¸ ConfiguraÃ§Ã£o

Para controlar o que serÃ¡ extraÃ­do, edite o arquivo `config.json` antes de executar:

```json
{
  "anos_para_processar": ["2024", "2023"],
  "prefeituras_para_processar": [
    "aracaju",
    "barra",
    "pirambu",
    "pacatuba"
  ],
  "configuracoes_paralelismo": {
    "max_workers": 4
  },
  "configuracoes_cidades": {
    "aracaju": {
      "scraper_module": "aracaju_barra_pirambu_scraper",
      "url": "https://www.municipioonline.com.br/se/prefeitura/aracaju/cidadao/despesa",
      "nome_iframe": null
    },
    "barra": {
      "scraper_module": "aracaju_barra_pirambu_scraper",
      "url": "https://www.municipioonline.com.br/se/prefeitura/barradoscoqueiros/cidadao/despesa",
      "nome_iframe": null
    },
    "pirambu": {
      "scraper_module": "aracaju_barra_pirambu_scraper",
      "url": "https://www.municipioonline.com.br/se/prefeitura/pirambu/cidadao/despesa",
      "nome_iframe": null
    },
    "pacatuba": {
      "scraper_module": "pacatuba_scraper",
      "url": "https://transparencia.pacatuba.se.gov.br/public/portal/despesas"
    }
  }
}
```
* **`anos_para_processar`**: Lista de anos para os quais o robÃ´ irÃ¡ rodar.
* **`prefeituras_para_processar`**: Lista de cidades que serÃ£o processadas. O nome deve corresponder a uma chave em `configuracoes_cidades`.
* **`max_workers`**: NÃºmero de tarefas paralelas (navegadores) a serem executadas ao mesmo tempo.

---
