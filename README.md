# ExDRoP - Extrator de Dados de Royalties do Petr√≥leo

## üìú Sobre o Projeto

O **ExDRoP** √© uma ferramenta de automa√ß√£o projetada para extrair, de forma robusta e paralela, dados de pagamentos de royalties de m√∫ltiplos portais da transpar√™ncia de munic√≠pios em Sergipe. O projeto nasceu da necessidade de contornar a falta de padroniza√ß√£o e a dificuldade de acesso a esses dados p√∫blicos, fornecendo um conjunto de dados limpo e consolidado para an√°lise e controle social.

O sistema conta com uma interface gr√°fica amig√°vel para configura√ß√£o e execu√ß√£o, al√©m de um modo de automa√ß√£o para uso em servidores.

## ‚ú® Funcionalidades

* **Extra√ß√£o Multi-portal:** Suporte para diferentes layouts de portais (fam√≠lia Serigy e outros).
* **Processamento Paralelo:** Utiliza m√∫ltiplos "workers" (threads) para acelerar drasticamente a extra√ß√£o de dados.
* **Robusto e Resiliente:** Possui l√≥gicas de retentativas para lidar com instabilidades dos portais, coleta em lotes para evitar travamentos e salvamento de diagn√≥stico em caso de falhas.
* **Interface Gr√°fica:** Um painel de controle constru√≠do com Streamlit permite a configura√ß√£o e o acompanhamento da extra√ß√£o em tempo real, de forma totalmente visual.
* **Containerizado com Docker:** Tanto o scraper quanto a interface podem ser executados via Docker, garantindo um ambiente consistente e facilitando a distribui√ß√£o.
* **Configura√ß√£o Flex√≠vel:** Todas as execu√ß√µes s√£o controladas por um arquivo `config.json`, permitindo selecionar cidades, anos e meses espec√≠ficos.

## üõ†Ô∏è Pr√©-requisitos

Para executar o projeto, voc√™ precisar√° de:

* **Python 3.11+**
* **Docker Desktop** (para a execu√ß√£o via containers, que √© a forma mais recomendada)

## üöÄ Como Executar o ExDRoP

Este projeto oferece m√∫ltiplas formas de execu√ß√£o, projetadas para diferentes tipos de usu√°rios, desde pesquisadores que desejam uma interface amig√°vel at√© desenvolvedores que precisam de automa√ß√£o em servidor.

### Modo 1: Usando a Interface Gr√°fica (Recomendado para Usu√°rios)

Esta √© a forma mais f√°cil de usar o extrator. Ela abre um painel de controle no seu navegador onde voc√™ pode configurar e iniciar a extra√ß√£o.

#### Op√ß√£o A (Mais Simples): Via Docker
Voc√™ s√≥ precisa do Docker Desktop instalado no seu computador.

1.  **Construa a imagem da UI (apenas na primeira vez):**
    ```bash
    # O -f aponta para o Dockerfile da interface
    docker build -t extrator-ui -f Dockerfile.ui .
    ```

2.  **Execute o container da UI:**
    ```powershell
    # Comando para Windows (PowerShell)
    docker run --rm -p 8501:8501 -v "$(pwd)/data:/app/data" -v "$(pwd)/logs:/app/logs" -v "$(pwd)/config.json:/app/config.json" extrator-ui
    ```

3.  **Acesse a Interface:**
    Abra seu navegador e acesse: **`http://localhost:8501`**

#### Op√ß√£o B (Para Desenvolvimento): Localmente com `venv`
Este m√©todo √© ideal para quem est√° a modificar o c√≥digo da interface.

1.  **Ative o ambiente virtual:**
    ```powershell
    # Para Windows (PowerShell)
    .\venv\Scripts\Activate
    ```
2.  **Instale as depend√™ncias (se necess√°rio):**
    ```bash
    pip install -r requirements.txt
    ```
3.  **Execute o Streamlit:**
    ```bash
    python -m streamlit run interface.py
    ```

### Modo 2: Execu√ß√£o Automatizada do Scraper (Recomendado para Servidores)

Este modo executa o rob√¥ em "headless" (sem interface gr√°fica), lendo a configura√ß√£o do `config.json`. √â ideal para ser agendado (`cron`) em um servidor.

1.  **Construa a imagem do scraper (apenas na primeira vez):**
    ```bash
    # O -f aponta para o Dockerfile do scraper
    docker build -t extrator-sergipe -f Dockerfile.scraper .
    ```

2.  **Execute o container do scraper:**
    ```powershell
    # Comando para Windows (PowerShell)
    docker run --rm -v "$(pwd)/data:/app/data" -v "$(pwd)/logs:/app/logs" -v "$(pwd)/config.json:/app/config.json" extrator-sergipe
    ```

    ## üìÇ Estrutura do Projeto
```
.
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ processed/      # Onde os arquivos CSV finais s√£o salvos
‚îÇ   ‚îî‚îÄ‚îÄ raw/            # (Opcional) Para dados brutos, se necess√°rio
‚îú‚îÄ‚îÄ logs/               # Arquivos de log detalhados da execu√ß√£o
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ common/         # M√≥dulos compartilhados (ex: logger)
‚îÇ   ‚îî‚îÄ‚îÄ scrapers/       # Cada scraper em seu pr√≥prio arquivo .py
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ config.json         # Arquivo principal de configura√ß√£o
‚îú‚îÄ‚îÄ Dockerfile.scraper  # Dockerfile para o rob√¥ de automa√ß√£o
‚îú‚îÄ‚îÄ Dockerfile.ui       # Dockerfile para a interface gr√°fica
‚îú‚îÄ‚îÄ interface.py        # C√≥digo da interface com Streamlit
‚îú‚îÄ‚îÄ main.py             # Ponto de entrada principal do rob√¥
‚îî‚îÄ‚îÄ requirements.txt    # Lista de depend√™ncias Python
```

## ‚öôÔ∏è Configura√ß√£o

A configura√ß√£o da extra√ß√£o √© feita de forma simples e visual atrav√©s da **Interface Gr√°fica**, n√£o sendo necess√°rio editar arquivos manualmente para o uso comum.

### Configurando pela Interface Gr√°fica

Ao executar a interface (`Modo 1` da se√ß√£o "Como Executar"), voc√™ encontrar√° as seguintes op√ß√µes no painel de controle:

* **Prefeituras para Processar:** Uma caixa de sele√ß√£o m√∫ltipla para escolher quais munic√≠pios ser√£o inclu√≠dos na extra√ß√£o.
* **Anos para Processar:** Um campo de texto onde voc√™ pode listar os anos desejados, separados por v√≠rgula.
* **Meses para Processar (Condicional):** Um campo de texto que aparece se voc√™ selecionar um munic√≠pio compat√≠vel (todos os atuais). Permite especificar meses (ex: `01, 02, 11`), otimizando a extra√ß√£o. Se deixado em branco, o rob√¥ processar√° o ano inteiro.
* **N√∫mero de Processos Paralelos (Workers):** Um slider para definir quantos navegadores rodar√£o simultaneamente. Uma ajuda (`?`) explica como escolher o melhor n√∫mero com base na capacidade do seu computador.
* **Executar em modo visual:** Uma caixa de sele√ß√£o que permite assistir √† execu√ß√£o do rob√¥. Ideal para depura√ß√£o.

Ao clicar em "Salvar Configura√ß√µes e Iniciar Extra√ß√£o", suas escolhas s√£o salvas automaticamente no arquivo `config.json` e a execu√ß√£o come√ßa.

### Para Desenvolvedores ou Automa√ß√£o (Editando o `config.json`)

Para execu√ß√µes automatizadas em servidor (`Modo 2`), a configura√ß√£o √© lida diretamente do arquivo `config.json`. Voc√™ pode edit√°-lo manualmente para controlar o processo.

```json
{
  "anos_para_processar": ["2024", "2023"],
  "prefeituras_para_processar": ["aracaju", "pacatuba"],
  "meses_para_processar": ["01", "02", "03"],
  "configuracoes_paralelismo": {
    "max_workers": 4
  },
  "configuracoes_cidades": {
    "aracaju": {
      "scraper_module": "aracaju_barra_pirambu_scraper",
      "url": "[https://www.municipioonline.com.br/se/prefeitura/aracaju/cidadao/despesa](https://www.municipioonline.com.br/se/prefeitura/aracaju/cidadao/despesa)"
    },
    "barra": {
      "scraper_module": "aracaju_barra_pirambu_scraper",
      "url": "[https://www.municipioonline.com.br/se/prefeitura/barradoscoqueiros/cidadao/despesa](https://www.municipioonline.com.br/se/prefeitura/barradoscoqueiros/cidadao/despesa)"
    },
    "pirambu": {
      "scraper_module": "aracaju_barra_pirambu_scraper",
      "url": "[https://www.municipioonline.com.br/se/prefeitura/pirambu/cidadao/despesa](https://www.municipioonline.com.br/se/prefeitura/pirambu/cidadao/despesa)"
    },
    "pacatuba": {
      "scraper_module": "pacatuba_scraper",
      "url": "[https://transparencia.pacatuba.se.gov.br/public/portal/despesas](https://transparencia.pacatuba.se.gov.br/public/portal/despesas)"
    }
  }
}
```

* anos_para_processar: Lista de anos para os quais o rob√¥ ir√° rodar.

* prefeituras_para_processar: Lista das chaves das cidades que ser√£o processadas.

* meses_para_processar (Opcional): Se presente, o rob√¥ processar√° apenas os meses listados para as cidades compat√≠veis. Se ausente ou null, processar√° o ano inteiro.

* max_workers: N√∫mero de tarefas paralelas (navegadores) a serem executadas ao mesmo tempo.

* configuracoes_cidades: Dicion√°rio com as configura√ß√µes espec√≠ficas de cada portal, como a URL e o m√≥dulo scraper a ser utilizado.


## üì¶ Manuten√ß√£o e Atualiza√ß√£o das Imagens

Para garantir que a aplica√ß√£o continue segura e est√°vel, √© recomendado reconstruir as imagens Docker periodicamente (a cada 1-2 meses) para incorporar as √∫ltimas atualiza√ß√µes de seguran√ßa da imagem base e das depend√™ncias.

O processo consiste em dois passos:

### 1. Atualizar a Imagem Base

Primeiro, garanta que voc√™ tenha a vers√£o mais recente da imagem oficial do Python que usamos como base:

```bash
docker pull python:3.11-slim
```

### 2. Reconstruir as Imagens da Aplica√ß√£o sem Cache
Em seguida, reconstrua as suas imagens `extrator-ui` e extrator-sergipe usando a flag --no-cache. Isso for√ßa o Docker a executar todos os passos do zero, incluindo o apt-get upgrade, garantindo que as √∫ltimas atualiza√ß√µes sejam aplicadas.

Para a imagem da Interface:

```bash

docker build --no-cache -t `extrator-ui` -f Dockerfile.ui .
```

Para a imagem do Scraper (automa√ß√£o):

```bash

docker build --no-cache -t extrator-sergipe -f Dockerfile.scraper .
```

